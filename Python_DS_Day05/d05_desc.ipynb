{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"iBJxV7kqcxhz","outputId":"ce0d8c9c-bc2c-4de0-b119-da97e67fca9e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649771995912,"user_tz":-180,"elapsed":3593,"user":{"displayName":"Анастасия Бубаева","userId":"02723908328637800899"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.7/dist-packages (0.9.1)\n","Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (2.4.417127.4579844)\n","Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.7.2)\n","Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n","Populating the interactive namespace from numpy and matplotlib\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":57}],"source":["import pandas as pd\n","import numpy as np\n","import warnings\n","warnings.filterwarnings('ignore')\n","from IPython.core.display import Image, display\n","%matplotlib inline\n","import seaborn as sns\n","from matplotlib import pyplot as plt\n","import pylab as pl\n","import re\n","import codecs\n","import nltk\n","!pip install pymorphy2    #\n","import pymorphy2\n","from sklearn.metrics import classification_report\n","from sklearn.linear_model import LogisticRegression\n","%pylab inline\n","pylab.rcParams['figure.figsize'] = (15,10)\n","import re                 #\n","import csv                #\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')"]},{"cell_type":"markdown","metadata":{"id":"O6guuzB4cxh4"},"source":["-----\n","\n","# Задание 1\n","\n","1. С помощью *pd.read_csv()* загрузите датафреймы positive.csv и negative.csv (обратите внимание, что исходные таблицы не содержат наименования столбцов и на первой строке располагаются данные. Файлы расположены в папке datasets);\n","2. Объедините датафреймы с помощью *pd.concat()* в один датафрейм;\n","3. Убедитесь, что в новом датафрейме индексация сквозная и без повторов;\n","4. Переименуйте столбцы датафрейма (столбцы полностью соответствуют примеру);\n","5. Выведите информацию об общем количестве полученных твитов, сколько из них негативных, сколько позитивных.\n","\n","-----"]},{"cell_type":"markdown","metadata":{"id":"IASQVA1mUE15"},"source":["---\n","\n","## Семантический анализ твитов\n","\n","Сегодня мы построим классификатор, который будет разделять текст на позитивные и негативные высказывания. Для этого мы воспользуемся уже размеченной базой.\n","Загрузим данные для анализа"]},{"cell_type":"code","source":["from google.colab import drive  # если вы выполняете код из среды Google Colab, нужно подключить свой гугл-диск,\n","drive.mount('/content/drive')   # чтобы можно было оттуда считать файл с данными для этого задания"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GeaqB6dpUgxm","executionInfo":{"status":"ok","timestamp":1649771997759,"user_tz":-180,"elapsed":1853,"user":{"displayName":"Анастасия Бубаева","userId":"02723908328637800899"}},"outputId":"a6873432-1c2b-4b80-eff5-075054d1525f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["col_names = ['id', 'date', 'name', 'text', 'positive', 'rep', 'rtv', 'fav', 'total_count', 'fol', 'friends', 'list_count']\n","df_pos = pd.read_csv('/content/drive/MyDrive/school21/day05/datasets/positive.csv', sep=';', names=col_names)\n","df_neg = pd.read_csv('/content/drive/MyDrive/school21/day05/datasets/negative.csv', sep=';', names=col_names)"],"metadata":{"id":"nQfOp-ApQjlX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_pos.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xtlCPGCXd9yD","executionInfo":{"status":"ok","timestamp":1649771999313,"user_tz":-180,"elapsed":11,"user":{"displayName":"Анастасия Бубаева","userId":"02723908328637800899"}},"outputId":"9e185707-f325-4685-8be1-c373ccbcd6e8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(114911, 12)"]},"metadata":{},"execution_count":60}]},{"cell_type":"code","source":["df_neg.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zgooBEdbeCq6","executionInfo":{"status":"ok","timestamp":1649771999313,"user_tz":-180,"elapsed":10,"user":{"displayName":"Анастасия Бубаева","userId":"02723908328637800899"}},"outputId":"eb23690e-eda9-4b69-8192-935d019f4938"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(111923, 12)"]},"metadata":{},"execution_count":61}]},{"cell_type":"code","source":["df = pd.concat([df_pos, df_neg], axis=0, ignore_index=True)"],"metadata":{"id":"B3fyPxWiXwmv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fYVZx8jAbsJD","executionInfo":{"status":"ok","timestamp":1649771999314,"user_tz":-180,"elapsed":8,"user":{"displayName":"Анастасия Бубаева","userId":"02723908328637800899"}},"outputId":"f3dca6f5-307a-4c2a-bd4f-308fec6cf830"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(226834, 12)"]},"metadata":{},"execution_count":63}]},{"cell_type":"code","source":["len(df.id.unique())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WdgO3-ukcyJ7","executionInfo":{"status":"ok","timestamp":1649771999314,"user_tz":-180,"elapsed":6,"user":{"displayName":"Анастасия Бубаева","userId":"02723908328637800899"}},"outputId":"2a47ddaf-6ad6-45ba-801e-0a45c94cf196"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["226834"]},"metadata":{},"execution_count":64}]},{"cell_type":"code","source":["len(df[df.id.duplicated() == True])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"stMpkx5Rb2w0","executionInfo":{"status":"ok","timestamp":1649771999594,"user_tz":-180,"elapsed":285,"user":{"displayName":"Анастасия Бубаева","userId":"02723908328637800899"}},"outputId":"567c2b6a-4518-4168-d0c7-27be79d8c84a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":65}]},{"cell_type":"code","source":["df.positive.unique()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PMacbUKPfUia","executionInfo":{"status":"ok","timestamp":1649771999594,"user_tz":-180,"elapsed":6,"user":{"displayName":"Анастасия Бубаева","userId":"02723908328637800899"}},"outputId":"cf51644e-b295-4073-a0f3-d42ff3cc50d9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 1, -1])"]},"metadata":{},"execution_count":66}]},{"cell_type":"markdown","metadata":{"id":"9lfaINJ1T2lS"},"source":["---\n","\n","Все колонки таблицы могут содержать информацию о тональности твита, но мы будем ориентироваться исключительно на текст и на столбец отнесения к классу positiv.\n","Заменим значение -1 в колонке positive на 0"]},{"cell_type":"code","source":["df.positive[df.positive == -1] = 0"],"metadata":{"id":"42H_WQpAgiLU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(df[df.id.duplicated() == True])"],"metadata":{"id":"YGURSh_LhWLg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649771999595,"user_tz":-180,"elapsed":5,"user":{"displayName":"Анастасия Бубаева","userId":"02723908328637800899"}},"outputId":"0d443056-028a-432a-d20d-db775e65300c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":68}]},{"cell_type":"markdown","source":["Количество строк в датафрейме соответствует количеству id, что также подтверждается нулевым значением количества дублирующих записей id. Таким образом, индексация сквозная и без повторов"],"metadata":{"id":"R-e7tWfVc3QL"}},{"cell_type":"markdown","source":["Сохраним объединенный датафрейм в таблицу excel."],"metadata":{"id":"uvdxfr5vUUNp"}},{"cell_type":"code","source":["nik_name = 'rriddler'\n","df.to_excel('/content/drive/MyDrive/school21/day05/datasets_'+nik_name+'.xlsx', index=False)"],"metadata":{"id":"-LqyFr_EUQTt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Всего количество полученных твитов', len(df.positive), ', из них негативных ', len(df.positive[df.positive == 0]), ', позитивных ', len(df.positive[df.positive == 1]), '.') #Всего количество полученных твитов 226834 , из них негативных  111923 , позитивных  114911 ."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CYI8zoCaYl0l","executionInfo":{"status":"ok","timestamp":1649772113249,"user_tz":-180,"elapsed":14,"user":{"displayName":"Анастасия Бубаева","userId":"02723908328637800899"}},"outputId":"a8f9991f-cfca-4f66-80c3-a146f7357362"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Всего количество полученных твитов 226834 , из них негативных  111923 , позитивных  114911 .\n"]}]},{"cell_type":"markdown","metadata":{"id":"rdNlfhdFSMx5"},"source":["----\n","\n","# Задание 2\n","\n","Произведите очистку данных, сформированных в задании 1. По результатам очистки выведите на экран следующую информацию:   \n","- Общее количество слов перед удалением слов, встречающихся 1 раз;\n","- Количество слов, встречающихся 1 раз;\n","- Итоговое количество слов;\n","- Количество пустых твитов;\n","- Из них позитивных твитов;\n","- Количество твитов после удаления пустых.\n","\n","----\n","\n","***Совет:*** сохраняйте промежуточные результаты очистки, чтобы в случае неверных действий на каком-либо этапе не пересчитывать все предыдущие этапы\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"BJ6Uu0l_cxh5"},"source":["### Очистка и предобработка данных\n","\n","Перед разработкой классификатора нам необходимо очистить и предобработать данные.\n","\n","Начнем с очистки данных\n","\n","----------------------------\n","\n","***Внимание!*** Библиотека [*nltk*](https://www.nltk.org) может содержать не все компоненты. В случае возникновения ошибки необходимо запустить скрипт\n","\n","*import nltk   \n","nltk.download()*\n","\n","В открывшемся окне необходимо выбрать и установить требуемые компоненты\n","\n","----------------------------"]},{"cell_type":"markdown","metadata":{"id":"dF-tFibCcxh6"},"source":["Приведем весь текст к строчным буквам:"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"mXe5Z7Btcxh6","outputId":"f1d25041-8f3e-452b-bebe-53fb74c3890e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649772113609,"user_tz":-180,"elapsed":370,"user":{"displayName":"Анастасия Бубаева","userId":"02723908328637800899"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["19    @moscow_advokat очень главное спасибо for   ме...\n","20    у нас есть прекрасная история, как сдохнуть за...\n","21    @benjamin1610 сегодня столько поводов выпить, ...\n","22    @stalingulag унылый?...наверное...вдруг его пе...\n","Name: text, dtype: object"]},"metadata":{},"execution_count":71}],"source":["df.text = df.text.str.lower()\n","df.text.loc[19:22]"]},{"cell_type":"markdown","metadata":{"id":"0Dj3naj-cxh8"},"source":["---\n","\n","Оставим в тексте только русские слова, удалив числа, знаки препинания, специальные символы и слова написанные латиницей:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FuLUqMv5cxh9","outputId":"b0a322d0-a66b-413c-d0b8-5fa953bdea14","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649772114808,"user_tz":-180,"elapsed":1202,"user":{"displayName":"Анастасия Бубаева","userId":"02723908328637800899"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["19                    очень главное спасибо       ме...\n","20    у нас есть прекрасная история  как сдохнуть за...\n","21                  сегодня столько поводов выпить  ...\n","22                 унылый    наверное   вдруг его пе...\n","Name: text, dtype: object"]},"metadata":{},"execution_count":72}],"source":["df.text = df.text.str.replace(r\"[^А-Яа-я]\",\" \")\n","df.text.loc[19:22]"]},{"cell_type":"markdown","metadata":{"id":"he1YbhVvcxh9"},"source":["---\n","\n","Мы анализируем русскоязычный твиттер, поэтому английские слова, а так же числа, будут представлять частные случаи и формировать шум в данных. Но могут возникнуть задачи, где удаляемые слова и числа важны. В этом случае потребуется более взвешенный подход к очистке. Вам могут помочь [константы модуля *string*](https://docs.python.org/3/library/string.html)"]},{"cell_type":"markdown","metadata":{"id":"UuCaIjPScxh-"},"source":["Разобьем тексты на слова с помощью *word_tokenize*:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vJA_VuZ5cxh-","outputId":"a5f12113-2999-4386-d54d-28f82ea2ca65","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649772145681,"user_tz":-180,"elapsed":30874,"user":{"displayName":"Анастасия Бубаева","userId":"02723908328637800899"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["19    [очень, главное, спасибо, медвед, он, работал,...\n","20    [у, нас, есть, прекрасная, история, как, сдохн...\n","21    [сегодня, столько, поводов, выпить, что, я, уж...\n","22    [унылый, наверное, вдруг, его, перед, этим, чп...\n","Name: text, dtype: object"]},"metadata":{},"execution_count":73}],"source":["from nltk.tokenize import word_tokenize\n","df.text = list(map(word_tokenize, df.text))\n","df.text.loc[19:22]"]},{"cell_type":"markdown","metadata":{"id":"6w3LiQIfcxh_"},"source":["---\n","\n","В каждом языке имеются так называемые стоп-слова - это, например, предлоги, союзы, местоимения и т.д. Стоп-слова не несут смысловой нагрузки, но при этом встречаются достаточно часто. Существует множество словарей стоп-слов, мы воспользуемся словарем библиотеки *nltk*. При решении конкретных задач вы можете как расширить словарь стоп-слов, так и удалить из него любые слова."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4focLj2Hcxh_","outputId":"8d540ee9-abe5-476f-baf1-4cbcac5b9603","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649772145682,"user_tz":-180,"elapsed":14,"user":{"displayName":"Анастасия Бубаева","userId":"02723908328637800899"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['а',\n"," 'без',\n"," 'более',\n"," 'больше',\n"," 'будет',\n"," 'будто',\n"," 'бы',\n"," 'был',\n"," 'была',\n"," 'были',\n"," 'было',\n"," 'быть',\n"," 'в',\n"," 'вам',\n"," 'вас',\n"," 'вдруг',\n"," 'ведь',\n"," 'во',\n"," 'вот',\n"," 'впрочем',\n"," 'все',\n"," 'всегда',\n"," 'всего',\n"," 'всех',\n"," 'всю',\n"," 'вы',\n"," 'где',\n"," 'да',\n"," 'даже',\n"," 'два',\n"," 'для',\n"," 'до',\n"," 'другой',\n"," 'его',\n"," 'ее',\n"," 'ей',\n"," 'ему',\n"," 'если',\n"," 'есть',\n"," 'еще',\n"," 'ж',\n"," 'же',\n"," 'за',\n"," 'зачем',\n"," 'здесь',\n"," 'и',\n"," 'из',\n"," 'или',\n"," 'им',\n"," 'иногда',\n"," 'их',\n"," 'к',\n"," 'как',\n"," 'какая',\n"," 'какой',\n"," 'когда',\n"," 'конечно',\n"," 'кто',\n"," 'куда',\n"," 'ли',\n"," 'лучше',\n"," 'между',\n"," 'меня',\n"," 'мне',\n"," 'много',\n"," 'может',\n"," 'можно',\n"," 'мой',\n"," 'моя',\n"," 'мы',\n"," 'на',\n"," 'над',\n"," 'надо',\n"," 'наконец',\n"," 'нас',\n"," 'не',\n"," 'него',\n"," 'нее',\n"," 'ней',\n"," 'нельзя',\n"," 'нет',\n"," 'ни',\n"," 'нибудь',\n"," 'никогда',\n"," 'ним',\n"," 'них',\n"," 'ничего',\n"," 'но',\n"," 'ну',\n"," 'о',\n"," 'об',\n"," 'один',\n"," 'он',\n"," 'она',\n"," 'они',\n"," 'опять',\n"," 'от',\n"," 'перед',\n"," 'по',\n"," 'под',\n"," 'после',\n"," 'потом',\n"," 'потому',\n"," 'почти',\n"," 'при',\n"," 'про',\n"," 'раз',\n"," 'разве',\n"," 'с',\n"," 'сам',\n"," 'свою',\n"," 'себе',\n"," 'себя',\n"," 'сейчас',\n"," 'со',\n"," 'совсем',\n"," 'так',\n"," 'такой',\n"," 'там',\n"," 'тебя',\n"," 'тем',\n"," 'теперь',\n"," 'то',\n"," 'тогда',\n"," 'того',\n"," 'тоже',\n"," 'только',\n"," 'том',\n"," 'тот',\n"," 'три',\n"," 'тут',\n"," 'ты',\n"," 'у',\n"," 'уж',\n"," 'уже',\n"," 'хорошо',\n"," 'хоть',\n"," 'чего',\n"," 'чем',\n"," 'через',\n"," 'что',\n"," 'чтоб',\n"," 'чтобы',\n"," 'чуть',\n"," 'эти',\n"," 'этого',\n"," 'этой',\n"," 'этом',\n"," 'этот',\n"," 'эту',\n"," 'я']"]},"metadata":{},"execution_count":74}],"source":["from nltk.corpus import stopwords\n","russian_stopwords = stopwords.words(\"russian\")\n","russian_stopwords.sort()\n","russian_stopwords"]},{"cell_type":"markdown","metadata":{"id":"stwsz6MFcxh_"},"source":["---\n","\n","Удалим стоп-слова из наших данных"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rGSeR8VNcxiA","outputId":"8d28e7eb-399d-4eb2-af5d-834049f4c9a7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649772151303,"user_tz":-180,"elapsed":5631,"user":{"displayName":"Анастасия Бубаева","userId":"02723908328637800899"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["19    [очень, главное, спасибо, медвед, работал, кло...\n","20              [прекрасная, история, сдохнуть, неделю]\n","21    [сегодня, столько, поводов, выпить, могу, усид...\n","22                   [унылый, наверное, этим, чпокнули]\n","Name: text, dtype: object"]},"metadata":{},"execution_count":75}],"source":["def delete_stopword(words):\n","    global russian_stopwords\n","    new_s = [word for word in words if word not in russian_stopwords]\n","    return new_s\n","\n","df.text = list(map(delete_stopword, df.text))\n","df.text.loc[19:22]"]},{"cell_type":"markdown","metadata":{"id":"m2bgmKDkcxiA"},"source":["---\n","\n","Проведем [лемматизацию](https://ru.wikipedia.org/wiki/Лемматизация) полученных слов"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"00mHnfBdcxiB","outputId":"4b0e9b13-6a7a-476c-d8e0-675e634090ff","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649772459825,"user_tz":-180,"elapsed":308532,"user":{"displayName":"Анастасия Бубаева","userId":"02723908328637800899"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["19    [очень, главное, спасибо, медведа, работать, к...\n","20              [прекрасный, история, сдохнуть, неделя]\n","21    [сегодня, столько, повод, выпить, мочь, усидет...\n","22                    [унылый, наверное, это, чпокнуль]\n","Name: text, dtype: object"]},"metadata":{},"execution_count":76}],"source":["import pymorphy2\n","morph = pymorphy2.MorphAnalyzer()\n","\n","def lemmatization(words):\n","    global morph\n","    new_s = [morph.parse(word)[0].normal_form for word in words]\n","    return new_s\n","\n","df.text = list(map(lemmatization, df.text))\n","df.text.loc[19:22]"]},{"cell_type":"code","source":["df.to_excel('/content/drive/MyDrive/school21/day05/datasets_lemm_'+nik_name+'.xlsx', index=False)"],"metadata":{"id":"Vskr8ZjYda0V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nROxZJjDcxiB"},"source":["---\n","\n","Теперь необходимо удалить все слова, которые встречаются только 1 раз"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UvoyixDGcxiB","outputId":"cf47477a-fda2-4d32-8175-72e6297df6ae","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649772538059,"user_tz":-180,"elapsed":11195,"user":{"displayName":"Анастасия Бубаева","userId":"02723908328637800899"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["19    очень главное спасибо медведа работать клоун а...\n","20                   прекрасный история сдохнуть неделя\n","21      сегодня столько повод выпить мочь усидеть место\n","22                                  унылый наверное это\n","Name: text, dtype: object"]},"metadata":{},"execution_count":78}],"source":["from nltk.probability import FreqDist\n","\n","def to_str(s):\n","    new_s = ' '.join(j for j in s)\n","    return new_s\n","\n","text_tokens = word_tokenize(' '.join(j for j in list(map(to_str, df.text))))\n","text = nltk.Text(text_tokens)\n","fdist = FreqDist(text)\n","words_to_del = list(filter(lambda k: fdist[k] == 1, fdist))\n","words_to_del = set(words_to_del)\n","\n","def delete_word(words):\n","    global words_to_del\n","    new_s = [word for word in words if word not in words_to_del]\n","    return new_s\n","\n","df.text = list(map(delete_word, df.text))\n","df.text = list(map(to_str, df.text))\n","df.text.loc[19:22]"]},{"cell_type":"code","source":["df.to_excel('/content/drive/MyDrive/school21/day05/datasets_del_'+nik_name+'.xlsx', index=False)"],"metadata":{"id":"6GFZ8GVNel-y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9J_Ei3tHcxiB"},"source":["---\n","\n","На этом очистка данных завершена. Можно ли утверждать, что очистка идеальна? Однозначно нет! Но, ее может оказаться достаточно для решения нашей задачи.   \n","Какие еще задачи могут возникнуть при очистке текстовых данных? Вот далеко неполный список:\n","- Обработка больших документов и больших коллекций текстовых документов, которые не помещаются в память.\n","- Извлечение текста из разметки, такой как HTML, PDF или другие структурированные форматы документов.\n","- Транслитерация символов с других языков.\n","- Декодирование символов Юникода в нормализованную форму, такую как UTF8\n","- Обработка доменных имен, фраз и сокращений.\n","- Обработка или удаление чисел, таких как даты и суммы.\n","- Поиск и исправление распространенных опечаток и ошибок в написании.\n","\n","Можно очень долго заниматься очисткой и не достичь идеального результата. Лучше подойти к задаче итеративно - осуществить стандартную очистку и посмотреть на результат, если результат недостаточный, то провести дополнительные мероприятия по очистке."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yu3rMjvscxiD","outputId":"b67672c9-d617-4f06-f2bb-38ef8002e819","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649772603761,"user_tz":-180,"elapsed":19,"user":{"displayName":"Анастасия Бубаева","userId":"02723908328637800899"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["1493275 - общее количество слов перед удалением слов, встречающихся 1 раз; 53568 – количество слов, встречающихся 1 раз; 1439707 – итоговое количество слов после очистки.\n"]}],"source":["print(len(text_tokens), '- общее количество слов перед удалением слов, встречающихся 1 раз;', len(words_to_del), '– количество слов, встречающихся 1 раз;',  len(text_tokens) - len(words_to_del), '– итоговое количество слов после очистки.') #(1493275, 53568, 1439707)"]},{"cell_type":"markdown","metadata":{"id":"Tc7MzjibcxiD"},"source":["---\n","После очистки могут оказаться пустые твиты, т.е. эти твиты состояли из слов, записанных латиницей, стоп-слов, чисел, знаков припинания и уникальных слов. Такие твиты необходимо удалить из данных:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PVJRoKsdcxiD","outputId":"c2177a71-78f9-45d3-c7ef-3f469ab97f1a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649772603761,"user_tz":-180,"elapsed":9,"user":{"displayName":"Анастасия Бубаева","userId":"02723908328637800899"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["856 – количество пустых твитов 382 – из них позитивных твитов.\n"]}],"source":["print(len(df[df.text == '']), '– количество пустых твитов', len(df[(df.text == '') & (df.positive == 1)]), '– из них позитивных твитов.') #(856, 382)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fJ_slJtrcxiE","outputId":"8412a843-c1bd-4a25-a373-31d23c609bca","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649772651134,"user_tz":-180,"elapsed":247,"user":{"displayName":"Анастасия Бубаева","userId":"02723908328637800899"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["225978 – количество твитов после удаления пустых\n"]}],"source":["df = df.drop(df[df.text == ''].index, axis = 0) #225978\n","print(len(df), '– количество твитов после удаления пустых')"]},{"cell_type":"markdown","metadata":{"id":"rc0oypVscc3G"},"source":["---\n","\n","# Задание 3\n","\n","1. Кодировать данные методом мешка слов.\n","2. Кодировать данные методом TF-IDF.\n","3. Построить классификатор на основе логистической регрессии, используя мешок слов.\n","4. Построить классификатор на основе логистической регрессии, используя TF-IDF.\n","5. Построить классификатор на основе случайного леса, используя мешок слов.\n","6. Построить классификатор на основе случайного леса, используя TF-IDF.\n","7. Сделайте выводы о разработанных классификаторах.\n","\n","---\n","\n","При разбиении на обучающую и тестовую выборки, следует указать *test_size=0.3*\n","\n","---\n","\n","***Рекомендация:*** для случайного леса параметр n_estimator должен быть не менее 200"]},{"cell_type":"markdown","metadata":{"id":"svDU8K6fcxiE"},"source":["Для разработки моделей нам необходимо оцифровать полученные данные. Мы воспользуемся двумя методами: мешком слов [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) и TF-IDF [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Но в начале необходимо разбить данные на обучающую и тестовую выборки.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XvjctSJgcxiE"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(df.text, df.positive, test_size=0.3, random_state=21)"]},{"cell_type":"markdown","metadata":{"id":"sRHONwlncxiF"},"source":["---\n","\n","Рассмотрим количество твитов в выборке для обучения, из них позитивных, и в выборке для теста, из них позитивных"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1sCJ_AalcxiF","outputId":"bc201095-d0bd-4b76-fe46-7983d1d43e9a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649762355021,"user_tz":-180,"elapsed":236,"user":{"displayName":"Анастасия Бубаева","userId":"02723908328637800899"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(158184, 80157, 67794, 34372)"]},"metadata":{},"execution_count":49}],"source":["len(y_train), y_train.sum(), len(y_test), y_test.sum()"]},{"cell_type":"markdown","metadata":{"id":"ozA-Cy-EcxiF"},"source":["---\n","\n","### Кодировка данных   \n","\n","Кодируем наши данные мешком слов и tf-idf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9gnjEXNCcxiG"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","\n","cv = CountVectorizer()\n","cv_train = cv.fit_transform(X_train)\n","cv_test = cv.transform(X_test)\n","\n","tfidf = TfidfVectorizer()\n","tfidf_train = tfidf.fit_transform(X_train)\n","tfidf_test = tfidf.transform(X_test)"]},{"cell_type":"markdown","metadata":{"id":"saY0gM8ScxiG"},"source":["---\n","\n","### Классификаторы\n","\n","Построим классификатор с помощью логистической регрессии:   \n","на основе мешка слов:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kDuFc9fXcxiG","outputId":"5a44c48d-1c30-4653-ad61-aabcc567637b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649762428804,"user_tz":-180,"elapsed":9362,"user":{"displayName":"Анастасия Бубаева","userId":"02723908328637800899"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["test\n","              precision    recall  f1-score   support\n","\n","           0       0.72      0.71      0.71     33422\n","           1       0.72      0.73      0.72     34372\n","\n","    accuracy                           0.72     67794\n","   macro avg       0.72      0.72      0.72     67794\n","weighted avg       0.72      0.72      0.72     67794\n","\n","train\n","              precision    recall  f1-score   support\n","\n","           0       0.80      0.79      0.79     78027\n","           1       0.80      0.80      0.80     80157\n","\n","    accuracy                           0.80    158184\n","   macro avg       0.80      0.80      0.80    158184\n","weighted avg       0.80      0.80      0.80    158184\n","\n"]}],"source":["lr = LogisticRegression(random_state=21)\n","lr.fit(cv_train, y_train)\n","cv_pred = lr.predict(cv_test)\n","print('test')\n","print(classification_report(y_test, cv_pred))\n","print('train')\n","print(classification_report(y_train, lr.predict(cv_train)))"]},{"cell_type":"markdown","metadata":{"id":"npaM2-TmcxiH"},"source":["---\n","\n","на основе tf-idf:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xG84z4XrcxiH","outputId":"d672db57-bbfc-4f42-e7da-fe84f543f103","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649762440694,"user_tz":-180,"elapsed":9556,"user":{"displayName":"Анастасия Бубаева","userId":"02723908328637800899"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["test\n","              precision    recall  f1-score   support\n","\n","           0       0.73      0.69      0.71     33422\n","           1       0.71      0.75      0.73     34372\n","\n","    accuracy                           0.72     67794\n","   macro avg       0.72      0.72      0.72     67794\n","weighted avg       0.72      0.72      0.72     67794\n","\n","train\n","              precision    recall  f1-score   support\n","\n","           0       0.79      0.75      0.77     78027\n","           1       0.77      0.80      0.79     80157\n","\n","    accuracy                           0.78    158184\n","   macro avg       0.78      0.78      0.78    158184\n","weighted avg       0.78      0.78      0.78    158184\n","\n"]}],"source":["lr = LogisticRegression(random_state=21)\n","lr.fit(tfidf_train, y_train)\n","tfidf_pred = lr.predict(tfidf_test)\n","print('test')\n","print(classification_report(y_test, tfidf_pred))\n","print('train')\n","print(classification_report(y_train, lr.predict(tfidf_train)))"]},{"cell_type":"markdown","metadata":{"id":"MFelSBv8cxiH"},"source":["---\n","\n","Видно, что модели переобучены - это следствие малого количества данных."]},{"cell_type":"markdown","metadata":{"id":"7C5MM6FDcxiI"},"source":["---\n","\n","Построим классификатор с помощью случайного леса [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html):   \n","на основе мешка слов:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0MgSxj3HcxiI","outputId":"2688463f-941a-4daa-c3e8-5e2471f9acfc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649764760740,"user_tz":-180,"elapsed":2316991,"user":{"displayName":"Анастасия Бубаева","userId":"02723908328637800899"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["test\n","              precision    recall  f1-score   support\n","\n","           0       0.68      0.76      0.72     33422\n","           1       0.74      0.65      0.69     34372\n","\n","    accuracy                           0.70     67794\n","   macro avg       0.71      0.71      0.70     67794\n","weighted avg       0.71      0.70      0.70     67794\n","\n","train\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     78027\n","           1       1.00      1.00      1.00     80157\n","\n","    accuracy                           1.00    158184\n","   macro avg       1.00      1.00      1.00    158184\n","weighted avg       1.00      1.00      1.00    158184\n","\n"]}],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","forest = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=21)\n","forest.fit(cv_train, y_train)\n","cv_pred = forest.predict(cv_test)\n","print('test')\n","print(classification_report(y_test, cv_pred))\n","print('train')\n","print(classification_report(y_train, forest.predict(cv_train)))"]},{"cell_type":"markdown","metadata":{"id":"03sG8rNacxiI"},"source":["---\n","\n","на основе tf-idf:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VMJm7XC4cxiI","outputId":"644e59be-07ea-4c3d-ec69-187a51d90f46","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649768291419,"user_tz":-180,"elapsed":1656696,"user":{"displayName":"Анастасия Бубаева","userId":"02723908328637800899"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["test\n","              precision    recall  f1-score   support\n","\n","           0       0.72      0.70      0.71     33422\n","           1       0.72      0.74      0.73     34372\n","\n","    accuracy                           0.72     67794\n","   macro avg       0.72      0.72      0.72     67794\n","weighted avg       0.72      0.72      0.72     67794\n","\n","train\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     78027\n","           1       1.00      1.00      1.00     80157\n","\n","    accuracy                           1.00    158184\n","   macro avg       1.00      1.00      1.00    158184\n","weighted avg       1.00      1.00      1.00    158184\n","\n"]}],"source":["forest.fit(tfidf_train, y_train)\n","tfidf_pred = forest.predict(tfidf_test)\n","print('test')\n","print(classification_report(y_test, tfidf_pred))\n","print('train')\n","print(classification_report(y_train, forest.predict(tfidf_train)))"]},{"cell_type":"markdown","metadata":{"id":"RZJbQ24AcxiJ"},"source":["#Выводы: \n","----\n","Классификатор на основе логистической регрессии при использовании tf-idf на тестовой выборке показал лучший f1-score, чем при использовании мешка слов.\n","----\n","Классификатор на основе случайного леса, используя мешок слов, показал больше значение f1-score, чем при использовании TF-IDF.\n","Модель на RandomForest возможно переобучена, она на обучающей выборке показала f1-score 1, что значительно выше метрик по результатам обучения логистической регрессией. Снижение гиперпараметра n_estimators с 200 на 100 не влияет на результат, снижение до 3 плавно ухудшает f1 с 0.7 до 0. "]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"d05_desc.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}